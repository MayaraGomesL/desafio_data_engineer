ğŸ“¦ Dasafio Data Engineer â†’ Pipeline â€” Bronze â†’ Silver â†’ Gold (Delta + Spark + Postgres)

Pipeline de ingestÃ£o e transformaÃ§Ã£o dos Dados Abertos do CNPJ (Receita Federal) usando PySpark e Delta Lake, orquestrado via Docker Compose, com persistÃªncia em camadas (Bronze/Silver/Gold) e carga final no PostgreSQL.

âœ¨ VisÃ£o Geral

Fonte: Ã­ndice pÃºblico da Receita Federal (dados_abertos_cnpj), download via HTTP (requests + BeautifulSoup).

Raw (arquivos): zips baixados e extraÃ­dos localmente (arquivos como: ...EMPRECSV, ...SOCIOCSV).

Bronze (Delta): camada de dados bruta, sem transformaÃ§Ã£o. IngestÃ£o particionada por ano_mes, schema aplicado, armazenamento em /app/data/bronze.

Silver (Delta): normalizaÃ§Ã£o (tipos, limpeza, padronizaÃ§Ãµes) e deduplicaÃ§Ã£o simples por chave antes do MERGE. Armazenamento em /app/data/silver.

Gold (Delta): agregaÃ§Ã£o final (gold_flat_emp_socios) e carga no PostgreSQL.

ğŸ“ Estrutura de Pastas
.
â”œâ”€ config/
â”‚  â”œâ”€ variables.py          # caminhos, JDBC, nomes de tabelas
â”‚  â””â”€ schemas.py            # schemas das tabelas Bronze e Silver
â”œâ”€ src/
â”‚  â”œâ”€ pipeline.py           # orquestra o fluxo end-to-end
â”‚  â”œâ”€ utils/
â”‚  â”‚  â”œâ”€ spark_session.py   # get_spark() com Delta e driver JDBC
â”‚  â”‚  â”œâ”€ utils.py           # helpers (HTTP session, unzip, read_delta, metadados)
â”‚  â”‚  â””â”€ download_html.py   # listar meses/zips, baixar e extrair
â”‚  â”œâ”€ bronze/
â”‚  â”‚  â””â”€ bronze_ingestion.py   # write_bronze_append
â”‚  â”œâ”€ silver/
â”‚  â”‚  â””â”€ silver_ingestion.py   # normalizaÃ§Ãµes + write_silver_merge (dropDuplicates)
â”‚  â””â”€ gold/
â”‚     â”œâ”€ transform_gold.py     # criar_gold_flat_emp_socios
â”‚     â””â”€ load_gold.py          # write_gold_to_postgres
â”œâ”€ data/                    # Armazenamento dos dados
â”‚  â”œâ”€ bronze/
â”‚  â”œâ”€ silver/
â”‚  â”œâ”€ gold/
â”‚  â””â”€ work/                 # zips, extraÃ§Ãµes temporÃ¡rias
â”œâ”€ jars/
â”‚  â””â”€ postgresql-42.7.3.jar # driver JDBC
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.yml
â”œâ”€ requirements.txt
â””â”€ README.md

ğŸ§± Camadas de Dados
RAW (arquivos)

Arquivos tÃ­picos: K3241.K03200Y1.D50913.EMPRECSV, ...SOCIOCSV (sem extensÃ£o .csv).

A funÃ§Ã£o de unzip jÃ¡ seleciona/renomeia para .csv ou retorna todos os arquivos para leitura Spark com .format("csv").

Bronze (Delta)
Local: /app/data/bronze/{empresas|socios}.
Particionamento: ano_mes (ex.: ano_mes=2025-09).
Esquemas aplicados a partir de config/schemas.py.

Silver (Delta)
Local: /app/data/silver/{empresas|socios}.
NormalizaÃ§Ãµes:
- cnpj: limpeza (remover nÃ£o dÃ­gitos), empty_as_null, upper, regexp_replace, casts.
- colunas especÃ­ficas por domÃ­nio (empresas/sÃ³cios).
DeduplicaÃ§Ã£o simples antes do MERGE:
- df_source = df_source.dropDuplicates(key_columns)

Chaves:
Empresas: ["cnpj"]
SÃ³cios: ["cnpj", "documento_socio","tipo_socio"]

MERGE (upsert) para garantir atualizaÃ§Ã£o incremental.

Gold (Delta)
Local: /app/data/gold/gold_flat_emp_socios.

â–¶ï¸ Como Rodar
0) PrÃ©-requisitos
Docker + Docker Compose
Rede ativa (para baixar dados da RFB)

1) Rodar o pipeline (um-shot)
# sintaxe
docker compose run --rm app python -m src.pipeline <YYYY-MM> <EMP_ID> <SOC_ID>

# exemplo
docker compose run --rm app python -m src.pipeline 2025-09 1 1

2) Ver dados rapidamente
# Bronze empresas do mÃªs
docker compose run --rm app python - <<'PY'
from pyspark.sql import SparkSession as S
s=S.builder.getOrCreate()
df=s.read.format("delta").load("/app/data/bronze/empresas")
df.where("ano_mes='2025-09'").show(20, False)
PY

3) Acessar PostgreSQL
# docker compose exec -it db psql -U postgres -d ml_data_engineer_sql 
SELECT * FROM gold_flat_emp_socios LIMIT 10;


ğŸ“ LicenÃ§a & Fontes
Dados: Receita Federal do Brasil â€” Dados Abertos do CNPJ.
