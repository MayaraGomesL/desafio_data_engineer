ğŸ“¦ CNPJ Pipeline â€” Bronze â†’ Silver â†’ Gold (Delta + Spark + Postgres)

Pipeline de ingestÃ£o e transformaÃ§Ã£o dos Dados Abertos do CNPJ (Receita Federal) usando PySpark e Delta Lake, orquestrado via Docker Compose, com persistÃªncia em camadas (Bronze/Silver/Gold) e carga final no PostgreSQL.

Se a imagem nÃ£o aparecer, mova o arquivo arquitetura-projeto.png baixado para docs/ e commite. Como fallback, hÃ¡ tambÃ©m um diagrama Mermaid no final do README.

âœ¨ VisÃ£o Geral

Fonte: Ã­ndice pÃºblico da RFB (dados_abertos_cnpj), varrido via HTTP (requests + BeautifulSoup).

Raw (arquivos): zips baixados e extraÃ­dos localmente (arquivos como ...EMPRECSV, ...SOCIOCSV).

Bronze (Delta): ingestÃ£o particionada por ano_mes, schema aplicado, armazenamento em /app/data/bronze.

Silver (Delta): normalizaÃ§Ã£o (tipos, limpeza, padronizaÃ§Ãµes) e deduplicaÃ§Ã£o simples por chave antes do MERGE.

Gold (Delta): agregaÃ§Ã£o final (gold_flat_emp_socios) e carga no PostgreSQL (staging â†’ upsert na final).

ğŸ“ Estrutura de Pastas (essencial)
.
â”œâ”€ config/
â”‚  â”œâ”€ variables.py          # caminhos, JDBC, nomes de tabelas e SQLs (DDL/UPSERT)
â”‚  â””â”€ schemas.py            # schemas das tabelas Bronze
â”œâ”€ src/
â”‚  â”œâ”€ pipeline.py           # orquestra o fluxo end-to-end
â”‚  â”œâ”€ utils/
â”‚  â”‚  â”œâ”€ spark_session.py   # get_spark() com Delta via pip + driver JDBC
â”‚  â”‚  â”œâ”€ utils.py           # helpers (HTTP session, unzip, read_delta, metadados)
â”‚  â”‚  â””â”€ download_html.py   # listar meses/zips, baixar e extrair
â”‚  â”œâ”€ bronze/
â”‚  â”‚  â””â”€ bronze_ingestion.py   # write_bronze_append (idempotente por partiÃ§Ã£o)
â”‚  â”œâ”€ silver/
â”‚  â”‚  â””â”€ silver_ingestion.py   # normalizaÃ§Ãµes + write_silver_merge (dropDuplicates)
â”‚  â””â”€ gold/
â”‚     â”œâ”€ transform_gold.py     # criar_gold_flat_emp_socios(...)
â”‚     â””â”€ load_gold.py          # write_gold_to_postgres(...)
â”œâ”€ data/                    # gerado em runtime (nÃ£o versionar)
â”‚  â”œâ”€ bronze/
â”‚  â”œâ”€ silver/
â”‚  â”œâ”€ gold/
â”‚  â””â”€ work/                 # zips, extraÃ§Ãµes temporÃ¡rias
â”œâ”€ jars/
â”‚  â””â”€ postgresql-42.7.3.jar # driver JDBC (recomendado via spark.jars)
â”œâ”€ Dockerfile
â”œâ”€ docker-compose.yml
â”œâ”€ requirements.txt
â””â”€ README.md


Dica: adicione data/ ao .gitignore para nÃ£o versionar os arquivos.

ğŸ§± Camadas de Dados
RAW (arquivos)

Local: WORK_PATH â†’ /app/data/work (zips em zips/, extraÃ§Ãµes em csv_empresas/ e csv_socios/).

Arquivos tÃ­picos: K3241.K03200Y1.D50913.EMPRECSV, ...SOCIOCSV (sem extensÃ£o .csv).

A funÃ§Ã£o de unzip jÃ¡ seleciona/renomeia para .csv ou retorna todos os arquivos para leitura Spark com .format("csv").

Bronze (Delta)

Local: /app/data/bronze/{empresas|socios}.

Particionamento: ano_mes (ex.: ano_mes=2025-09).

Esquemas aplicados a partir de config/schemas.py.

Idempotente por partiÃ§Ã£o: recomendado escrever com

.mode("overwrite").option("replaceWhere", f"ano_mes='{ano_mes}'")


para reprocessamentos do mesmo mÃªs sem duplicar.

Silver (Delta)

Local: /app/data/silver/{empresas|socios}.

NormalizaÃ§Ãµes:

cnpj: limpeza (remover nÃ£o dÃ­gitos), empty_as_null, upper, regexp_replace, casts.

colunas especÃ­ficas por domÃ­nio (empresas/sÃ³cios).

DeduplicaÃ§Ã£o simples antes do MERGE:

df_source = df_source.dropDuplicates(key_columns)


Chaves tÃ­picas:

Empresas: ["cnpj"]

SÃ³cios: ["cnpj", "documento_socio"]

MERGE (upsert) para garantir atualizaÃ§Ã£o incremental.

Gold (Delta)

Local: /app/data/gold/gold_flat_emp_socios (exemplo).

criar_gold_flat_emp_socios(...) combina dimensÃµes/atributos para consumo analÃ­tico.

Carga no PostgreSQL:

Staging via JDBC (mode("overwrite").option("truncate","true")).

Upsert na final com psycopg2 (DDL_FINAL + UPSERT_SQL definidos em variables.py).

â–¶ï¸ Como Rodar
0) PrÃ©-requisitos

Docker + Docker Compose

Rede ativa (para baixar dados da RFB)

Driver JDBC do Postgres disponÃ­vel:

Baixe para o projeto: ./jars/postgresql-42.7.3.jar

O get_spark() jÃ¡ aponta para /app/jars/postgresql-42.7.3.jar via spark.jars

1) Subir sÃ³ o banco (opcional)
docker compose up -d db

2) Rodar o pipeline (um-shot)
# sintaxe
docker compose run --rm app python -m src.pipeline <YYYY-MM> <EMP_ID> <SOC_ID>

# exemplo
docker compose run --rm app python -m src.pipeline 2025-09 7 7


Dica: o compose tambÃ©m aceita command com variÃ¡veis:
command: ["python", "src/pipeline.py", "${ANO_MES:-2025-09}", "7", "7"]

3) Ver dados rapidamente
# Bronze empresas do mÃªs
docker compose run --rm app python - <<'PY'
from pyspark.sql import SparkSession as S
s=S.builder.getOrCreate()
df=s.read.format("delta").load("/app/data/bronze/empresas")
df.where("ano_mes='2025-09'").show(20, False)
PY

4) HistÃ³rico Delta
docker compose run --rm app python - <<'PY'
from delta.tables import DeltaTable
from pyspark.sql import SparkSession as S
s=S.builder.getOrCreate()
DeltaTable.forPath(s, "/app/data/bronze/empresas").history().show(50, False)
PY

âš™ï¸ ConfiguraÃ§Ã£o
config/variables.py (exemplo)
from pathlib import Path

# Data Lake
DATA_LAKE_PATH = Path("/app/data")
BRONZE_PATH = DATA_LAKE_PATH / "bronze"
SILVER_PATH = DATA_LAKE_PATH / "silver"
GOLD_PATH   = DATA_LAKE_PATH / "gold"

# TemporÃ¡rios
WORK_PATH        = DATA_LAKE_PATH / "work"
ZIPS_PATH        = WORK_PATH / "zips"
CSV_EMPRESAS_PATH= WORK_PATH / "csv_empresas"
CSV_SOCIOS_PATH  = WORK_PATH / "csv_socios"

# Fonte RFB
RF_BASE_URL = "https://arquivos.receitafederal.gov.br/dados/cnpj/dados_abertos_cnpj/"

# JDBC / Postgres
PG_HOST="db"; PG_PORT=5432; PG_DB="ml_data_engineer_sql"
PG_USER="postgres"; PG_PASSWORD="postgres"
PG_GOLD_TABLE="gold_flat_emp_socios"
# opcional: PG_JDBC_URL="jdbc:postgresql://db:5432/ml_data_engineer_sql"

# SQLs (exemplos)
DDL_FINAL = """
CREATE TABLE IF NOT EXISTS gold_flat_emp_socios (
  cnpj TEXT PRIMARY KEY,
  razao_social TEXT,
  tipo_socio INT,
  documento_socio TEXT,
  ano_mes TEXT
);
"""
UPSERT_SQL = """
INSERT INTO gold_flat_emp_socios (cnpj, razao_social, tipo_socio, documento_socio, ano_mes)
SELECT cnpj, razao_social, tipo_socio, documento_socio, ano_mes
FROM gold_flat_emp_socios_stg s
ON CONFLICT (cnpj)
DO UPDATE SET
  razao_social = EXCLUDED.razao_social,
  tipo_socio = EXCLUDED.tipo_socio,
  documento_socio = EXCLUDED.documento_socio,
  ano_mes = EXCLUDED.ano_mes;
"""

src/utils/spark_session.py (trecho chave)
from pyspark.sql import SparkSession
from delta import configure_spark_with_delta_pip

def get_spark():
    builder = (
        SparkSession.builder
        .appName("ml_data_engineer_spark")
        .config("spark.sql.extensions", "io.delta.sql.DeltaSparkSessionExtension")
        .config("spark.sql.catalog.spark_catalog", "org.apache.spark.sql.delta.catalog.DeltaCatalog")
        .config("spark.sql.session.timeZone", "America/Sao_Paulo")
        # driver JDBC local (recomendado e determinÃ­stico)
        .config("spark.jars", "/app/jars/postgresql-42.7.3.jar")
    )
    return configure_spark_with_delta_pip(builder).getOrCreate()

ğŸ”„ IdempotÃªncia e Reprocessamento

Bronze: use mode("overwrite") + replaceWhere por ano_mes para reprocessar o mesmo mÃªs sem duplicar.

Silver: dropDuplicates(key_columns) antes do MERGE para evitar conflitos.

Gold: staging overwrite + truncate, depois UPSERT para final.

ğŸ› ï¸ Problemas Comuns

â€œNo suitable driverâ€ na escrita JDBC
â†’ Garanta o JAR do Postgres no classpath (/app/jars/postgresql-42.7.3.jar + spark.jars).
Teste rÃ¡pido:

s._sc._jvm.java.lang.Class.forName("org.postgresql.Driver")


Sem CSVs apÃ³s unzip
â†’ Os arquivos da RFB nÃ£o tÃªm .csv. O unzip jÃ¡ lida com ...EMPRECSV/...SOCIOCSV.
Confira logs de extraÃ§Ã£o.

Merge com mÃºltiplas linhas por chave
â†’ Ative dropDuplicates por ["cnpj"] (empresas) e ["cnpj","documento_socio"] (sÃ³cios).

ğŸ“ LicenÃ§a & Fontes

Dados: Receita Federal do Brasil â€” Dados Abertos do CNPJ.

Este repositÃ³rio Ã© apenas um pipeline de ingestÃ£o/transformaÃ§Ã£o para fins educacionais e analÃ­ticos.